# ==================================================================================
# DOCKER COMPOSE FOR MARKER3 - SIMPLIFIED WITH BUILT-IN MODEL MANAGEMENT
# ==================================================================================
# Modes: dev-open → dev-secure → production
# Uses marker's built-in model management (no custom scripts)
# Docker Desktop compatible networking + RTX 5090 optimized
# ==================================================================================

version: '3.8'

services:

  # Development Open: First-time setup and model downloads
  marker-dev-open:
    image: marker3:rtx5090-dev-full
    container_name: marker3-dev-open
    profiles: [dev-open, first-run]
    volumes:
      - ./cache:/home/appuser/.cache  # Host directory for model cache (better permissions)
      - ./data:/app/data:rw
      - ./output:/app/output:rw
      # Live code editing for development
      - ./marker:/app/marker:rw
      - ./scripts:/app/scripts:rw
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONUNBUFFERED=1
      - HF_HUB_OFFLINE=0         # Allow model downloads
      - TRANSFORMERS_OFFLINE=0   # Allow model downloads
      - NETWORK_MODE=open
      - PYTHONPATH=/app
      - FASTAPI_RELOAD=true      # Enable hot reload for development
      - OLLAMA_BASE_URL=http://ollama:11434  # Connect to Ollama container
    runtime: nvidia
    networks:
      - default
      - postgresql_rag_network  # Shared network with Ollama container
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"  # FastAPI server
      - "8501:8501"  # Streamlit UI
    command: python -c "from marker.scripts.server import server_cli; server_cli(['--host', '0.0.0.0', '--port', '8000'])"
    healthcheck:
      test: ["CMD", "python", "-c", "import marker; print('Dev-open ready')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Longer startup time for model downloads

  # Development Secure: Live code editing with network restrictions
  marker-dev-secure:
    image: marker3:rtx5090-dev-full
    container_name: marker3-dev-secure
    profiles: [dev-secure, dev]
    volumes:
      - ./cache:/home/appuser/.cache  # Host directory for model cache
      - ./data:/app/data:rw
      - ./output:/app/output:rw
      # Live code editing for development
      - ./marker:/app/marker:rw
      - ./scripts:/app/scripts:rw
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONUNBUFFERED=1
      - HF_HUB_OFFLINE=1         # Secure - use cached models only
      - TRANSFORMERS_OFFLINE=1   # Secure - no external access
      - NETWORK_MODE=secure
      - PYTHONPATH=/app
      - OLLAMA_BASE_URL=http://ollama:11434  # Connect to Ollama container
    runtime: nvidia
    networks:
      - default
      - postgresql_rag_network  # Shared network with Ollama container
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"  # FastAPI server
      - "8501:8501"  # Streamlit UI
    command: python -c "from marker.scripts.server import server_cli; server_cli(['--host', '0.0.0.0', '--port', '8000'])"
    healthcheck:
      test: ["CMD", "python", "-c", "import marker; print('Dev-secure ready')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Secure Production V1: Secure configuration for sensitive data processing
  # Security Model: Dual-network (localhost access + future internal services)
  marker-secure-production-v1:
    image: marker3:rtx5090-dev-full  # Same image as dev, different security config
    container_name: marker-secure-production-v1
    profiles: [secure-production-v1]
    volumes:
      - ./cache:/home/appuser/.cache:ro  # Read-only cached models (no downloads)
      - ./marker:/app/marker:ro           # Read-only code access (shares dev code)
      - ./scripts:/app/scripts:ro
      - ./static:/app/static:rw           # Fonts and assets (needs write to download fonts first time)
      - ./data:/app/data:rw
      - ./output:/app/output:rw
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONUNBUFFERED=1
      # Security: Application-level blocks on external services
      - HF_HUB_OFFLINE=1         # Block model downloads
      - TRANSFORMERS_OFFLINE=1   # Block external ML services
      - NETWORK_MODE=secure-production
      - OLLAMA_BASE_URL=http://ollama:11434  # Connect to Ollama container
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "127.0.0.1:1111:8000"  # Localhost-only binding (blocks external host access)
    networks:
      - default  # For host connectivity (port mapping)
      - secure   # For future internal services (databases, caches)
      - postgresql_rag_network  # Shared network with Ollama container
    command: python -c "from marker.scripts.server import server_cli; server_cli(['--host', '0.0.0.0', '--port', '8000'])"
    # Maximum security hardening
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    security_opt:
      - no-new-privileges:true
    read_only: false
    tmpfs:
      - /tmp:rw,noexec,nosuid,size=1G
    healthcheck:
      test: ["CMD", "python", "-c", "import marker; print('Secure Production V1 ready')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

# Docker Desktop Compatible Networking
networks:
  default:
    driver: bridge
    # Docker Desktop's built-in network - development use (full internet access)
  secure:
    driver: bridge
    internal: true
    # Air-gapped network for secure production processing (no external access)
  postgresql_rag_network:
    external: true
    # Shared network with Ollama container for LLM communication

# Persistent data storage
volumes:
  marker_cache:
    driver: local  # Persistent cache for marker's built-in model management
  data:
    driver: local
  output:
    driver: local
